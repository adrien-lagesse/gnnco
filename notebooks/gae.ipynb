{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlagesse/gnnco/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import statistics\n",
    "from gnnco.models import GAT\n",
    "import gnnco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, val_dataset, train_loader, val_loader) = gnnco.graph_matching.setup_data(dataset_path=\"data/ER[100,8,0.02]/\", batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplacian_layer = gnnco.models.LaplacianEmbeddings(k=32)\n",
    "model = GAT(5, 32, 1024, 1024) #torch.nn.Sequential(torch.nn.Linear(32,4000), torch.nn.ReLU(), torch.nn.Linear(4000,4000), torch.nn.ReLU(), torch.nn.Linear(4000,256))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0: loss: 0.5403260678052902   acc:0.4665077455341816 (0.8125974103808403/0.436376778781414)\n",
      "val 0: loss: 0.14454811215400695   acc:0.5156167984008789 (0.8426230192184448/0.4872541785240173)\n",
      "train 1: loss: 0.1082688931375742   acc:0.518251147866249 (0.8903661578893661/0.4858589418232441)\n",
      "val 1: loss: 0.09405350387096405   acc:0.5226575851440429 (0.9216888189315796/0.48804795145988467)\n",
      "train 2: loss: 0.09120682142674923   acc:0.5211089998483658 (0.9365929141640663/0.48494444563984873)\n",
      "val 2: loss: 0.08897249400615692   acc:0.5248504042625427 (0.944975733757019/0.4884117305278778)\n",
      "train 3: loss: 0.08806038163602352   acc:0.5240160450339317 (0.9494688391685486/0.4869835816323757)\n",
      "val 3: loss: 0.08703812956809998   acc:0.5256012082099915 (0.9538374066352844/0.4884593904018402)\n",
      "train 4: loss: 0.08648472987115383   acc:0.5270135968923568 (0.9561781838536263/0.4896586000919342)\n",
      "val 4: loss: 0.0857650324702263   acc:0.5303272008895874 (0.9588751792907715/0.49315961003303527)\n",
      "train 5: loss: 0.08529230821877717   acc:0.5307395458221436 (0.9605085462331772/0.4933314502239227)\n",
      "val 5: loss: 0.08465485423803329   acc:0.532836401462555 (0.9633850216865539/0.495492684841156)\n",
      "train 6: loss: 0.08447738979011774   acc:0.5329572498798371 (0.9638695746660233/0.4954496771097183)\n",
      "val 6: loss: 0.08390299528837204   acc:0.5371127963066101 (0.9640716552734375/0.5000815093517303)\n",
      "train 7: loss: 0.0835674399510026   acc:0.5368130952119827 (0.9669980689883232/0.499368691444397)\n",
      "val 7: loss: 0.08323914706707   acc:0.537585997581482 (0.968285858631134/0.5002306222915649)\n",
      "train 8: loss: 0.08288905452936887   acc:0.5395988449454308 (0.9692773059010505/0.5021987743675709)\n",
      "val 8: loss: 0.08244026154279709   acc:0.5398372054100037 (0.971662187576294/0.5023829102516174)\n",
      "train 9: loss: 0.08233581148087979   acc:0.5418803974986076 (0.9713562145829201/0.5044978745281696)\n",
      "val 9: loss: 0.08193880170583726   acc:0.5438071966171265 (0.9730034828186035/0.5065823554992676)\n",
      "train 10: loss: 0.08273675795644522   acc:0.5396011486649513 (0.9723764583468437/0.5019305996596813)\n",
      "val 10: loss: 0.08171792328357697   acc:0.543288791179657 (0.9752625823020935/0.5058227181434631)\n",
      "train 11: loss: 0.08150572683662176   acc:0.5463376462459564 (0.9739666879177094/0.5091163292527199)\n",
      "val 11: loss: 0.08120632916688919   acc:0.545931589603424 (0.9753766298294068/0.5086844086647033)\n",
      "train 12: loss: 0.0807786675170064   acc:0.548701348900795 (0.9761804789304733/0.5114925846457481)\n",
      "val 12: loss: 0.0805855244398117   acc:0.5491168022155761 (0.9773826956748962/0.511971914768219)\n",
      "train 13: loss: 0.08038575444370508   acc:0.5509468451142311 (0.9773296490311623/0.5138335898518562)\n",
      "val 13: loss: 0.07988587021827698   acc:0.5530359864234924 (0.9793709516525269/0.5160589694976807)\n",
      "train 14: loss: 0.08027782514691353   acc:0.5513244971632958 (0.9780333384871482/0.5141837492585182)\n",
      "val 14: loss: 0.08049522787332535   acc:0.5439567923545837 (0.9791117191314698/0.5062144160270691)\n",
      "train 15: loss: 0.07986471429467201   acc:0.5532088488340378 (0.9791416570544242/0.516134463250637)\n",
      "val 15: loss: 0.07967095226049423   acc:0.5551275968551636 (0.9803697347640992/0.5182451725006103)\n",
      "train 16: loss: 0.07914059609174728   acc:0.557601447403431 (0.98016357421875/0.5208209782838822)\n",
      "val 16: loss: 0.07892541438341141   acc:0.5593159914016723 (0.9800848245620728/0.5228219151496887)\n",
      "train 17: loss: 0.07920438721776009   acc:0.5562322497367859 (0.9806123062968254/0.5192926809191704)\n",
      "val 17: loss: 0.0788669928908348   acc:0.5574640035629272 (0.9817012071609497/0.5206684231758117)\n",
      "train 18: loss: 0.07866307757794858   acc:0.5595315471291542 (0.981317688524723/0.5228181809186936)\n",
      "val 18: loss: 0.07837143391370774   acc:0.5594576120376586 (0.9827643632888794/0.5227428913116455)\n",
      "train 19: loss: 0.0793311771005392   acc:0.5563075467944145 (0.9820297300815582/0.5192517787218094)\n",
      "val 19: loss: 0.07857341021299362   acc:0.560722005367279 (0.9830263972282409/0.5240945816040039)\n",
      "train 20: loss: 0.07799066789448261   acc:0.5636259496212006 (0.9829327955842018/0.5271282777190208)\n",
      "val 20: loss: 0.07776393592357636   acc:0.5641788005828857 (0.9836826682090759/0.5277941226959229)\n",
      "train 21: loss: 0.07797019630670547   acc:0.5632406994700432 (0.9834432810544967/0.5266648039221764)\n",
      "val 21: loss: 0.07947355657815933   acc:0.5539183974266052 (0.9843270659446717/0.5165876030921936)\n",
      "train 22: loss: 0.07884191330522299   acc:0.5565296024084091 (0.9836666107177734/0.5193501338362694)\n",
      "val 22: loss: 0.07757658362388611   acc:0.5641983985900879 (0.9846975684165955/0.5277269840240478)\n",
      "train 23: loss: 0.07783737108111381   acc:0.5644278973340988 (0.9840750366449356/0.5279011696577072)\n",
      "val 23: loss: 0.0796909362077713   acc:0.5574428081512451 (0.9817631125450135/0.5206405520439148)\n",
      "train 24: loss: 0.07775034327059985   acc:0.5635286465287208 (0.9846939459443093/0.5268696263432503)\n",
      "val 24: loss: 0.07711014151573181   acc:0.5670979857444763 (0.9859634757041931/0.530768609046936)\n",
      "train 25: loss: 0.07674344182014466   acc:0.569268348813057 (0.9856785595417022/0.5330230683088303)\n",
      "val 25: loss: 0.0766163632273674   acc:0.5702036142349243 (0.985884714126587/0.5341504216194153)\n",
      "train 26: loss: 0.07680099867284298   acc:0.5680684477090836 (0.9862443104386329/0.5316701233386993)\n",
      "val 26: loss: 0.0766945093870163   acc:0.5664172053337098 (0.9861528515815735/0.5300125479698181)\n",
      "train 27: loss: 0.07626828886568546   acc:0.5715219989418984 (0.9866530075669289/0.5353878527879715)\n",
      "val 27: loss: 0.07612814456224441   acc:0.5706760048866272 (0.986879289150238/0.5345775723457337)\n",
      "train 28: loss: 0.0762700216844678   acc:0.5708387494087219 (0.9867342233657836/0.5346381366252899)\n",
      "val 28: loss: 0.07605008631944657   acc:0.5712835907936096 (0.9876344084739686/0.5351724624633789)\n",
      "train 29: loss: 0.07660678047686816   acc:0.5704222440719604 (0.986880923807621/0.5341723516583443)\n",
      "val 29: loss: 0.08074647337198257   acc:0.5464023947715759 (0.9839658260345459/0.5084501028060913)\n",
      "train 30: loss: 0.07776304017752408   acc:0.5634161964058876 (0.9858878999948502/0.5266420856118202)\n",
      "val 30: loss: 0.07574758231639862   acc:0.57289959192276 (0.9877937197685241/0.5369135975837708)\n",
      "train 31: loss: 0.0753761202096939   acc:0.5763593941926957 (0.9880725890398026/0.5405224531888961)\n",
      "val 31: loss: 0.07528364807367324   acc:0.5763868093490601 (0.98827565908432/0.5406632423400879)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    batch: gnnco.graph_matching.GMDatasetBatch\n",
    "    model.train()\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    edge_accuracy = []\n",
    "    nonedge_accuracy = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = batch.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "\n",
    "        #laplacian_embeddings = laplacian_layer.forward(batch.base_signals, batch.base_graphs).x()\n",
    "        final_embeddings = model.forward(batch.base_signals, batch.base_graphs).x().reshape((len(batch), 100, 1024))\n",
    "        prediction = torch.bmm(final_embeddings, torch.transpose(final_embeddings,1,2))\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        target = batch.base_graphs.to_dense().get_stacked_adj().float()\n",
    "        loss = (1/13)*(-12*target.flatten()*torch.log(prediction.flatten() + 1e-7) - (1-target.flatten())*torch.log(1-prediction.flatten() + 1e-7)).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(float(loss))\n",
    "        prediction = prediction > 0.5\n",
    "        target = target > 0.5\n",
    "        accuracy.append(float(torch.count_nonzero(prediction == target)/torch.numel(target)))\n",
    "        edge_accuracy.append(float(torch.count_nonzero(prediction[target])/ torch.count_nonzero(target)))\n",
    "        prediction = torch.logical_not(prediction)\n",
    "        target = torch.logical_not(target)\n",
    "        nonedge_accuracy.append(float(torch.count_nonzero(prediction[target])/ torch.count_nonzero(target)))\n",
    "\n",
    "    print(f\"train {epoch}: loss: {statistics.mean(losses)}   acc:{statistics.mean(accuracy)} ({statistics.mean(edge_accuracy)}/{statistics.mean(nonedge_accuracy)})\")\n",
    "\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    edge_accuracy = []\n",
    "    nonedge_accuracy = []\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        batch = batch.to(DEVICE)\n",
    "        # laplacian_embeddings = laplacian_layer.forward(batch.base_signals, batch.base_graphs).x()\n",
    "        final_embeddings = model.forward(batch.base_signals, batch.base_graphs).x().reshape((len(batch), 100, 1024))\n",
    "        prediction = torch.bmm(final_embeddings, torch.transpose(final_embeddings,1,2))\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        target = batch.base_graphs.to_dense().get_stacked_adj().float()\n",
    "        loss = (1/13)*(-12*target.flatten()*torch.log(prediction.flatten() + 1e-7) - (1-target.flatten())*torch.log(1-prediction.flatten() + 1e-7)).mean()\n",
    "        losses.append(float(loss))\n",
    "        prediction = prediction > 0.5\n",
    "        target = target > 0.5\n",
    "        accuracy.append(float(torch.count_nonzero(prediction == target)/torch.numel(target)))\n",
    "        edge_accuracy.append(float(torch.count_nonzero(prediction[target])/ torch.count_nonzero(target)))\n",
    "        prediction = torch.logical_not(prediction)\n",
    "        target = torch.logical_not(target)\n",
    "        nonedge_accuracy.append(float(torch.count_nonzero(prediction[target])/ torch.count_nonzero(target)))\n",
    "    print(f\"val {epoch}: loss: {statistics.mean(losses)}   acc:{statistics.mean(accuracy)} ({statistics.mean(edge_accuracy)}/{statistics.mean(nonedge_accuracy)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
