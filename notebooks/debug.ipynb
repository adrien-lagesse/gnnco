{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-inf, 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([-float('inf'), 0])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =torch.randn((5,5))\n",
    "mask = torch.tensor([True]*3 + [False]*2, dtype=torch.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, mask, n):\n",
    "    _, indices = torch.sort(\n",
    "        torch.masked_fill(\n",
    "            x, torch.logical_not(mask), -float(\"inf\")\n",
    "        ),\n",
    "        descending=True,\n",
    "    )\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20000):\n",
    "    xs =torch.randn((2,5,5))\n",
    "    masks = torch.stack([torch.tensor([True]*3 + [False]*2, dtype=torch.bool), torch.tensor([True]*4 + [False]*1, dtype=torch.bool)])\n",
    "    batched_f = torch.vmap(f, in_dims=(0,0,None))     \n",
    "    batched_f(xs, masks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __top_k_accuracy(\n",
    "    alignement_similarity: torch.FloatTensor, mask: torch.BoolTensor, top_n:int\n",
    ") -> torch.FloatTensor:\n",
    "    _, indices = torch.sort(\n",
    "        torch.masked_fill(\n",
    "            alignement_similarity, torch.logical_not(mask), -float(\"inf\")\n",
    "        ),\n",
    "        descending=True,\n",
    "    )\n",
    "    mask = mask.float()\n",
    "    m = torch.isin(torch.arange(len(alignement_similarity)), indices[:,:top_n]).float().squeeze()\n",
    "    acc = (m*mask).sum()/(mask.sum())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4])\n",
      "tensor([1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlagesse/.tmp/ipykernel_277571/3988777898.py:11: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::isin.Tensor_Tensor. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  m = torch.isin(torch.arange(len(alignement_similarity)), indices[:,:top_n]).float().squeeze()\n"
     ]
    }
   ],
   "source": [
    "batched_f = torch.vmap(__top_k_accuracy, in_dims=(0,0,None))  \n",
    "for _ in range(1):\n",
    "    xs =torch.randn((2,5,5))\n",
    "    masks = torch.stack([torch.tensor([True]*3 + [False]*2, dtype=torch.bool), torch.tensor([True]*4 + [False]*1, dtype=torch.bool)])\n",
    "    print(masks.sum(dim=1))\n",
    "    print(batched_f(xs, masks, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
